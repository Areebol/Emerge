{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "from utils.process_data import get_model_generate\n",
    "from data_loader.base_loader import BaseLoader\n",
    "# from data_processor.base_processor import BaseProcessor\n",
    "from data_loader.cot_loader import CotLoader\n",
    "from utils.load_config import load_config\n",
    "import argparse\n",
    "from utils.load_model import load_model_tokenizer\n",
    "import data_loader\n",
    "# import data_processor\n",
    "from utils.meter import AverageMeter\n",
    "from utils.process_data import *\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_entropy_token = AverageMeter()\n",
    "def process_data_token_level(model_generate):\n",
    "    # logging.info(f\"{self.name} process data\")\n",
    "    res_entropy = model_generate['entropy']\n",
    "    # num_input_tokens = res_entropy[0].__len__()\n",
    "    # num_heads = res_entropy.shape[0]\n",
    "    mean_entropy = res_entropy[:,1:].mean()\n",
    "    # total_entropy_token.update(mean_entropy)\n",
    "    return mean_entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_entropy = AverageMeter()\n",
    "def process_data_sen_level(model, tokenizer,data, model_generate,split_words=None):\n",
    "    # logging.info(f\"{self.name} process data\")\n",
    "    res = model_generate['generate']\n",
    "    input_ids = model_generate['input_ids']\n",
    "    encoder = get_encoder_k(model,-1)\n",
    "\n",
    "    # 划分输入        \n",
    "    if split_words:\n",
    "        split_tokens = split_sentence(tokenizer=tokenizer,question=data,input_ids=input_ids,split_words=split_words)\n",
    "    else:\n",
    "        split_tokens = split_sentence(tokenizer=tokenizer,question=data,input_ids=input_ids)\n",
    "        \n",
    "    # 根据句子切分attention矩阵 weight权重，token_ids 权重对应token下标\n",
    "    weights,token_ids = split_attn_matrix(model,res,split_tokens,soft_max=True)\n",
    "    \n",
    "    # 加权计算embedding得到hidden_states\n",
    "    hidden_states = weighted_hidden_states(weights,token_ids,res)        \n",
    "\n",
    "    # 计算attention矩阵\n",
    "    attn_matrix = get_attention_matrix(encoder,hidden_states).squeeze(0).to(torch.float32)\n",
    "    \n",
    "    # 计算entropy\n",
    "    with torch.no_grad():\n",
    "        sentence_entropy = get_attention_entropy(attn_matrix.cpu())\n",
    "        mean_sentence_entropy = torch.mean(sentence_entropy,dim=0).squeeze()\n",
    "    return mean_sentence_entropy.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, model, tokenizer, model_config, data_loaders:list[BaseLoader]):\n",
    "        logging.info(\"Init Pipeline\")\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer \n",
    "        self.model_config = model_config\n",
    "        self.data_loaders = data_loaders\n",
    "        # self.data_processors = data_processors\n",
    "        self.min_input_token = 100\n",
    "        self.max_input_token = 2000\n",
    "        self.max_sample = 50\n",
    "\n",
    "    def run(self):\n",
    "        logging.info(\"Pipeline start\")\n",
    "        # data_loaders\n",
    "        for data_loader in self.data_loaders:\n",
    "            logging.info(f\"Data loader {data_loader.name}\")\n",
    "            load_data = data_loader.load_data()\n",
    "            split_words = data_loader.split_words()\n",
    "            # init processor\n",
    "            # for data_processor in self.data_processors:\n",
    "            #     data_processor.set(data_loader.name)\n",
    "            index = 0\n",
    "            # data samples\n",
    "            for data in load_data:\n",
    "                inputs = self.tokenizer(data, padding=False, return_tensors='pt')\n",
    "                num_input_token = inputs['input_ids'].shape[1]\n",
    "                if num_input_token < self.min_input_token or num_input_token > self.max_input_token:\n",
    "                    logging.info(f\"num_input_token {num_input_token} less than min_input_token {self.min_input_token} or greater than max_input_token {self.max_input_token}\")\n",
    "                    continue\n",
    "                # pre process\n",
    "                model_generate = get_model_generate(self.tokenizer,self.model,data,max_new_tokens=1,max_input_token=400,split_words=split_words)\n",
    "                index += 1\n",
    "\n",
    "                total_entropy = process_data_token_level(model_generate)\n",
    "                print(f\"{index} total_entropy:\",total_entropy)\n",
    "                # process_data_sen_level(self.model,self.tokenizer,data,model_generate,split_words=split_words)\n",
    "\n",
    "                if index > self.max_sample:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     23\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m load_model_tokenizer(model_config\u001b[38;5;241m=\u001b[39mmodel_config)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# data loaders + data processors\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# data_loaders = [getattr(data_loader,loader_name)() for loader_name in config['data_loaders']]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# data_processors = [getattr(data_processor,processor_name)(model, tokenizer, model_config) for processor_name in config['data_processors']]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# init pipeline\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m data_loaders \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mgetattr\u001b[39m(data_loader,config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_loaders\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])()]\n\u001b[1;32m     30\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(model,tokenizer,model_config,data_loaders)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_wait_suspend(thread, frame, event, arg)\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdo_wait_suspend(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--cfg\", default=\"./config/qwen.yaml\", help=\"config file path\")\n",
    "parser.add_argument(\"--start\", default = 2, type=int, help=\"config file path\")\n",
    "parser.add_argument(\"--model_cfg\", default=\"./config/models_pz.yaml\", help=\"model config file path\")\n",
    "# args = parser.parse_args()\n",
    "args =parser.parse_known_args()[0]\n",
    "\n",
    "# log_f = '%(asctime)s | %(filename)s[line:%(lineno)d] | %(levelname)s | %(message)s'\n",
    "# logging.basicConfig(level=\"DEBUG\", format=log_f)\n",
    "# logging.basicConfig(level=\"INFO\", format=log_f)\n",
    "\n",
    "# load config \n",
    "config = load_config(args.cfg)\n",
    "model_cfg = load_config(args.model_cfg)\n",
    "\n",
    "model_familys = config['model_familys']\n",
    "model_configs = []\n",
    "for key in model_familys:\n",
    "    model_configs += model_cfg[f\"paths_{key}\"]\n",
    "\n",
    "# models\n",
    "for model_config in model_configs[args.start:args.start+2]:\n",
    "    model, tokenizer = load_model_tokenizer(model_config=model_config)\n",
    "\n",
    "    # data loaders + data processors\n",
    "    # data_loaders = [getattr(data_loader,loader_name)() for loader_name in config['data_loaders']]\n",
    "    # data_processors = [getattr(data_processor,processor_name)(model, tokenizer, model_config) for processor_name in config['data_processors']]\n",
    "    # init pipeline\n",
    "    data_loaders = [getattr(data_loader,config['data_loaders'][-1])()]\n",
    "    pipeline = Pipeline(model,tokenizer,model_config,data_loaders)\n",
    "    # run\n",
    "    pipeline.run()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
