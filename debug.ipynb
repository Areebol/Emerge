{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "from utils.process_data import get_model_generate\n",
    "from data_loader.base_loader import BaseLoader\n",
    "# from data_processor.base_processor import BaseProcessor\n",
    "from data_loader.cot_loader import CotLoader\n",
    "from utils.load_config import load_config\n",
    "import argparse\n",
    "from utils.load_model import load_model_tokenizer\n",
    "import data_loader\n",
    "# import data_processor\n",
    "from utils.meter import AverageMeter\n",
    "from utils.process_data import *\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_entropy_token = AverageMeter()\n",
    "def process_data_token_level(model,model_generate,soft_max=True,avg_head=True):\n",
    "    res = model_generate['generate']\n",
    "    encoder = get_encoder_k(model,-1)\n",
    "    hidden_state = get_hidden_state_k(res,-2)\n",
    "    # 手动计算最后一层的attention weight\n",
    "    attentions = get_attention_matrix(encoder,hidden_state,soft_max=soft_max).to(torch.float32).cpu() # shape = (bs_size,#heads,len,len)\n",
    "    res_entropy = get_attention_entropy(attentions,avg_head=avg_head,soft_max=soft_max) # shape = (bs_size,len)\n",
    "    mean_entropy = res_entropy[:,:,1:].mean()\n",
    "    return mean_entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_entropy = AverageMeter()\n",
    "def process_data_sen_level(model, tokenizer,data, model_generate,split_words=None,soft_max=True,avg_head=True):\n",
    "    # logging.info(f\"{self.name} process data\")\n",
    "    res = model_generate['generate']\n",
    "    input_ids = model_generate['input_ids']\n",
    "    encoder = get_encoder_k(model,-1)\n",
    "\n",
    "    # 划分输入        \n",
    "    if split_words:\n",
    "        split_tokens = split_sentence(tokenizer=tokenizer,question=data,input_ids=input_ids,split_words=split_words)\n",
    "    else:\n",
    "        split_tokens = split_sentence(tokenizer=tokenizer,question=data,input_ids=input_ids)\n",
    "        \n",
    "    # 根据句子切分attention矩阵 weight权重，token_ids 权重对应token下标\n",
    "    weights,token_ids = split_attn_matrix(model,res,split_tokens,soft_max=soft_max)\n",
    "    \n",
    "    # 加权计算embedding得到hidden_states\n",
    "    hidden_states = weighted_hidden_states(weights,token_ids,res)        \n",
    "\n",
    "    # 计算attention矩阵\n",
    "    attn_matrix = get_attention_matrix(encoder,hidden_states,soft_max=soft_max).to(torch.float32)\n",
    "    \n",
    "    # 计算entropy\n",
    "    with torch.no_grad():\n",
    "        sentence_entropy = get_attention_entropy(attn_matrix.cpu(),soft_max=soft_max,avg_head=avg_head)\n",
    "        mean_sentence_entropy = torch.mean(sentence_entropy,dim=0).squeeze()\n",
    "    return mean_sentence_entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_with_dots(text,tokenizer):\n",
    "\n",
    "    # 使用正则表达式找到 A: 和 The answer is 之间的部分\n",
    "    pattern = re.compile(r'(?<=A: ).*?(?=The answer is)', re.DOTALL)\n",
    "    matches = pattern.findall(text)\n",
    "\n",
    "    for match in matches:\n",
    "        # 使用 tokenizer 获取 tokens\n",
    "        tokens = tokenizer(match, padding=False, return_tensors='pt')['input_ids']\n",
    "        token_count = tokens.size(1)  # 获取 token 的数量\n",
    "\n",
    "        # 替换为等数量的省略号，每个省略号之间有空格\n",
    "        replacement = '... ' * token_count\n",
    "        replacement = replacement.strip()  # 移除最后一个多余的空格\n",
    "        text = text.replace(match, replacement)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entropy_scatter(data_lists, labels, x_label, y_label, title, save_path):\n",
    "    \"\"\"\n",
    "    绘制散点图并保存\n",
    "\n",
    "    参数：\n",
    "    - data_lists: 数据列表的列表，每个子列表对应一组数据\n",
    "    - labels: 数据标签的列表\n",
    "    - x_label: 横坐标标签\n",
    "    - y_label: 纵坐标标签\n",
    "    - title: 图表标题\n",
    "    - save_path: 保存路径\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for data_list, label in zip(data_lists, labels):\n",
    "        plt.scatter(list(range(len(data_list))), data_list, label=label)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# 创建路径并绘制图表\n",
    "def create_and_plot(model_name, soft_max, avg_head, nocot_list, dotcot_list, cot_list, level):\n",
    "    path = f'./debug_vis/soft_max_{soft_max}__avg_head_{avg_head}/{level}_level'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    data_lists = [nocot_list, dotcot_list, cot_list]\n",
    "    labels = [f'nocot_{level}', f'dotcot_{level}', f'cot_{level}']\n",
    "    x_label = 'Index Example'\n",
    "    y_labels = {\n",
    "        'token': 'Token Entropy',\n",
    "        'sen': 'Sentence Entropy',\n",
    "        'rio': 'Entropy Rio'\n",
    "    }\n",
    "    title = f'{y_labels[level]} Scatter of {model_name}'\n",
    "    save_path = f'{path}/{model_name}_{level}_entropy.png'\n",
    "\n",
    "    plot_entropy_scatter(data_lists, labels, x_label, y_labels[level], title, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, model, tokenizer, model_config, data_loaders:list[BaseLoader], soft_max=True, avg_head=True):\n",
    "        logging.info(\"Init Pipeline\")\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer \n",
    "        self.model_config = model_config\n",
    "        self.data_loaders = data_loaders\n",
    "        # self.data_processors = data_processors\n",
    "        self.min_input_token = 100\n",
    "        self.max_input_token = 2000\n",
    "        self.max_sample = 21\n",
    "\n",
    "        self.soft_max = soft_max\n",
    "        self.avg_head = avg_head\n",
    "\n",
    "    def run(self):\n",
    "        logging.info(\"Pipeline start\")\n",
    "        # data_loaders\n",
    "        for data_loader in self.data_loaders:\n",
    "            logging.info(f\"Data loader {data_loader.name}\")\n",
    "            load_data = data_loader.load_data()\n",
    "            split_words = data_loader.split_words()\n",
    "            # init processor\n",
    "            # for data_processor in self.data_processors:\n",
    "            #     data_processor.set(data_loader.name)\n",
    "            index = 0\n",
    "            nocot_token_list = []\n",
    "            dotcot_token_list = []\n",
    "            cot_token_list = []\n",
    "            nocot_sen_list = []\n",
    "            dotcot_sen_list = []\n",
    "            cot_sen_list = []\n",
    "            nocot_rio_list = []\n",
    "            dotcot_rio_list = []\n",
    "            cot_rio_list = []\n",
    "            # data samples\n",
    "            for data in load_data:\n",
    "                if (index+1)%3==2:\n",
    "                    data = replace_with_dots(data, self.tokenizer)\n",
    "                inputs = self.tokenizer(data, padding=False, return_tensors='pt')\n",
    "                num_input_token = inputs['input_ids'].shape[1]\n",
    "                # if num_input_token < self.min_input_token or num_input_token > self.max_input_token:\n",
    "                #     logging.info(f\"num_input_token {num_input_token} less than min_input_token {self.min_input_token} or greater than max_input_token {self.max_input_token}\")\n",
    "                #     continue\n",
    "                # pre process\n",
    "                model_generate = get_model_generate(self.tokenizer,self.model,data,max_new_tokens=1,max_input_token=400,split_words=split_words)\n",
    "                index += 1\n",
    "\n",
    "                total_entropy = process_data_token_level(self.model,model_generate,soft_max=self.soft_max,avg_head=self.avg_head)\n",
    "                sentence_entropy = process_data_sen_level(self.model,self.tokenizer,data,model_generate,split_words=split_words,soft_max=self.soft_max,avg_head=self.avg_head)\n",
    "                if (index)%3==1:\n",
    "                    nocot_token_list.append(total_entropy)\n",
    "                    nocot_sen_list.append(sentence_entropy)\n",
    "                    nocot_rio_list.append(total_entropy/sentence_entropy)\n",
    "                    # print(f\"{index // 3+1} entropy_rio for nocot:\",total_entropy/sentence_entropy)\n",
    "                elif (index)%3==2:\n",
    "                    dotcot_token_list.append(total_entropy)\n",
    "                    dotcot_sen_list.append(sentence_entropy)\n",
    "                    dotcot_rio_list.append(total_entropy/sentence_entropy)\n",
    "                    # print(f\"{index // 3+1} entropy_rio for nocot:\",total_entropy/sentence_entropy)\n",
    "                else:\n",
    "                    cot_token_list.append(total_entropy)\n",
    "                    cot_sen_list.append(sentence_entropy)\n",
    "                    cot_rio_list.append(total_entropy/sentence_entropy)\n",
    "                    # print(f\"{index // 3} entropy_rio for nocot:\",total_entropy/sentence_entropy)                                      \n",
    "                # process_data_sen_level(self.model,self.tokenizer,data,model_generate,split_words=split_words)\n",
    "\n",
    "                if index >= self.max_sample:\n",
    "                    break\n",
    "            # 绘制散点图\n",
    "            create_and_plot(self.model_config[0], self.soft_max, self.avg_head, nocot_token_list, dotcot_token_list, cot_token_list, 'token')\n",
    "            create_and_plot(self.model_config[0], self.soft_max, self.avg_head, nocot_sen_list, dotcot_sen_list, cot_sen_list, 'sen')\n",
    "            create_and_plot(self.model_config[0], self.soft_max, self.avg_head, nocot_rio_list, dotcot_rio_list, cot_rio_list, 'rio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--cfg\", default=\"./config/llama2.yaml\", help=\"config file path\")\n",
    "parser.add_argument(\"--start\", default = 5, type=int, help=\"config file path\")\n",
    "parser.add_argument(\"--model_cfg\", default=\"./config/models_pz.yaml\", help=\"model config file path\")\n",
    "# args = parser.parse_args()\n",
    "args =parser.parse_known_args()[0]\n",
    "\n",
    "# log_f = '%(asctime)s | %(filename)s[line:%(lineno)d] | %(levelname)s | %(message)s'\n",
    "# logging.basicConfig(level=\"DEBUG\", format=log_f)\n",
    "# logging.basicConfig(level=\"INFO\", format=log_f)\n",
    "\n",
    "# load config \n",
    "config = load_config(args.cfg)\n",
    "model_cfg = load_config(args.model_cfg)\n",
    "\n",
    "model_familys = config['model_familys']\n",
    "model_configs = []\n",
    "for key in model_familys:\n",
    "    model_configs += model_cfg[f\"paths_{key}\"]\n",
    "\n",
    "# models\n",
    "for model_config in model_configs[args.start:args.start+1]:\n",
    "    print(\"model_name:\",model_config[0])\n",
    "    model, tokenizer = load_model_tokenizer(model_config=model_config)\n",
    "\n",
    "    # data loaders + data processors\n",
    "    # data_loaders = [getattr(data_loader,loader_name)() for loader_name in config['data_loaders']]\n",
    "    # data_processors = [getattr(data_processor,processor_name)(model, tokenizer, model_config) for processor_name in config['data_processors']]\n",
    "    # init pipeline\n",
    "    data_loaders = [getattr(data_loader,config['data_loaders'][0])()]\n",
    "    pipeline = Pipeline(model,tokenizer,model_config,data_loaders,soft_max=True, avg_head=True)\n",
    "    # run\n",
    "    pipeline.run()\n",
    "\n",
    "    pipeline = Pipeline(model,tokenizer,model_config,data_loaders,soft_max=True, avg_head=False)\n",
    "    # run\n",
    "    pipeline.run()\n",
    "\n",
    "    pipeline = Pipeline(model,tokenizer,model_config,data_loaders,soft_max=False, avg_head=True)\n",
    "    # run\n",
    "    pipeline.run()\n",
    "\n",
    "    pipeline = Pipeline(model,tokenizer,model_config,data_loaders,soft_max=False, avg_head=False)\n",
    "    # run\n",
    "    pipeline.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
