{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "from utils.process_data import get_model_generate\n",
    "from data_loader.base_loader import BaseLoader\n",
    "# from data_processor.base_processor import BaseProcessor\n",
    "from data_loader.cot_loader import CotLoader\n",
    "from utils.load_config import load_config\n",
    "import argparse\n",
    "from utils.load_model import load_model_tokenizer\n",
    "import data_loader\n",
    "# import data_processor\n",
    "from utils.meter import AverageMeter\n",
    "from utils.process_data import *\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_entropy_token = AverageMeter()\n",
    "def process_data_token_level(model_generate):\n",
    "    # logging.info(f\"{self.name} process data\")\n",
    "    res_entropy = model_generate['entropy']\n",
    "    # num_input_tokens = res_entropy[0].__len__()\n",
    "    # num_heads = res_entropy.shape[0]\n",
    "    mean_entropy = res_entropy[:,1:].mean()\n",
    "    # total_entropy_token.update(mean_entropy)\n",
    "    return mean_entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_entropy = AverageMeter()\n",
    "def process_data_sen_level(model, tokenizer,data, model_generate,split_words=None):\n",
    "    # logging.info(f\"{self.name} process data\")\n",
    "    res = model_generate['generate']\n",
    "    input_ids = model_generate['input_ids']\n",
    "    encoder = get_encoder_k(model,-1)\n",
    "\n",
    "    # 划分输入        \n",
    "    if split_words:\n",
    "        split_tokens = split_sentence(tokenizer=tokenizer,question=data,input_ids=input_ids,split_words=split_words)\n",
    "    else:\n",
    "        split_tokens = split_sentence(tokenizer=tokenizer,question=data,input_ids=input_ids)\n",
    "        \n",
    "    # 根据句子切分attention矩阵 weight权重，token_ids 权重对应token下标\n",
    "    weights,token_ids = split_attn_matrix(model,res,split_tokens,soft_max=True)\n",
    "    \n",
    "    # 加权计算embedding得到hidden_states\n",
    "    hidden_states = weighted_hidden_states(weights,token_ids,res)        \n",
    "\n",
    "    # 计算attention矩阵\n",
    "    attn_matrix = get_attention_matrix(encoder,hidden_states).squeeze(0).to(torch.float32)\n",
    "    \n",
    "    # 计算entropy\n",
    "    with torch.no_grad():\n",
    "        sentence_entropy = get_attention_entropy(attn_matrix.cpu())\n",
    "        mean_sentence_entropy = torch.mean(sentence_entropy,dim=0).squeeze()\n",
    "    return mean_sentence_entropy.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, model, tokenizer, model_config, data_loaders:list[BaseLoader]):\n",
    "        logging.info(\"Init Pipeline\")\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer \n",
    "        self.model_config = model_config\n",
    "        self.data_loaders = data_loaders\n",
    "        # self.data_processors = data_processors\n",
    "        self.min_input_token = 100\n",
    "        self.max_input_token = 2000\n",
    "        self.max_sample = 50\n",
    "\n",
    "    def run(self):\n",
    "        logging.info(\"Pipeline start\")\n",
    "        # data_loaders\n",
    "        for data_loader in self.data_loaders:\n",
    "            logging.info(f\"Data loader {data_loader.name}\")\n",
    "            load_data = data_loader.load_data()\n",
    "            split_words = data_loader.split_words()\n",
    "            # init processor\n",
    "            # for data_processor in self.data_processors:\n",
    "            #     data_processor.set(data_loader.name)\n",
    "            index = 0\n",
    "            # data samples\n",
    "            for data in load_data:\n",
    "                inputs = self.tokenizer(data, padding=False, return_tensors='pt')\n",
    "                num_input_token = inputs['input_ids'].shape[1]\n",
    "                if num_input_token < self.min_input_token or num_input_token > self.max_input_token:\n",
    "                    logging.info(f\"num_input_token {num_input_token} less than min_input_token {self.min_input_token} or greater than max_input_token {self.max_input_token}\")\n",
    "                    continue\n",
    "                # pre process\n",
    "                model_generate = get_model_generate(self.tokenizer,self.model,data,max_new_tokens=1,max_input_token=400,split_words=split_words)\n",
    "                index += 1\n",
    "\n",
    "                total_entropy = process_data_token_level(model_generate)\n",
    "                print(f\"{index} total_entropy:\",total_entropy)\n",
    "                # process_data_sen_level(self.model,self.tokenizer,data,model_generate,split_words=split_words)\n",
    "\n",
    "                if index > self.max_sample:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 total_entropy: 0.3948315382003784\n",
      "2 total_entropy: 0.3926246166229248\n",
      "3 total_entropy: 0.3896627724170685\n",
      "4 total_entropy: 0.3960888087749481\n",
      "5 total_entropy: 0.39916566014289856\n",
      "6 total_entropy: 0.40624886751174927\n",
      "7 total_entropy: 0.3913097679615021\n",
      "8 total_entropy: 0.37745583057403564\n",
      "9 total_entropy: 0.3804439306259155\n",
      "10 total_entropy: 0.391831636428833\n",
      "11 total_entropy: 0.38374462723731995\n",
      "12 total_entropy: 0.3948315382003784\n",
      "13 total_entropy: 0.37445247173309326\n",
      "14 total_entropy: 0.3627113103866577\n",
      "15 total_entropy: 0.3896627724170685\n",
      "16 total_entropy: 0.3721194267272949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 total_entropy: 0.4672941565513611\n",
      "2 total_entropy: 0.4699353277683258\n",
      "3 total_entropy: 0.46484386920928955\n",
      "4 total_entropy: 0.4744057357311249\n",
      "5 total_entropy: 0.47569918632507324\n",
      "6 total_entropy: 0.47659623622894287\n",
      "7 total_entropy: 0.4679994583129883\n",
      "8 total_entropy: 0.44949495792388916\n",
      "9 total_entropy: 0.45339128375053406\n",
      "10 total_entropy: 0.467337965965271\n",
      "11 total_entropy: 0.45842763781547546\n",
      "12 total_entropy: 0.4672941565513611\n",
      "13 total_entropy: 0.4467427134513855\n",
      "14 total_entropy: 0.4470313787460327\n",
      "15 total_entropy: 0.46484386920928955\n",
      "16 total_entropy: 0.4554743766784668\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--cfg\", default=\"./config/qwen.yaml\", help=\"config file path\")\n",
    "parser.add_argument(\"--start\", default = 2, type=int, help=\"config file path\")\n",
    "parser.add_argument(\"--model_cfg\", default=\"./config/models_pz.yaml\", help=\"model config file path\")\n",
    "# args = parser.parse_args()\n",
    "args =parser.parse_known_args()[0]\n",
    "\n",
    "# log_f = '%(asctime)s | %(filename)s[line:%(lineno)d] | %(levelname)s | %(message)s'\n",
    "# logging.basicConfig(level=\"DEBUG\", format=log_f)\n",
    "# logging.basicConfig(level=\"INFO\", format=log_f)\n",
    "\n",
    "# load config \n",
    "config = load_config(args.cfg)\n",
    "model_cfg = load_config(args.model_cfg)\n",
    "\n",
    "model_familys = config['model_familys']\n",
    "model_configs = []\n",
    "for key in model_familys:\n",
    "    model_configs += model_cfg[f\"paths_{key}\"]\n",
    "\n",
    "# models\n",
    "for model_config in model_configs[args.start:args.start+2]:\n",
    "    model, tokenizer = load_model_tokenizer(model_config=model_config)\n",
    "\n",
    "    # data loaders + data processors\n",
    "    # data_loaders = [getattr(data_loader,loader_name)() for loader_name in config['data_loaders']]\n",
    "    # data_processors = [getattr(data_processor,processor_name)(model, tokenizer, model_config) for processor_name in config['data_processors']]\n",
    "    # init pipeline\n",
    "    data_loaders = [getattr(data_loader,config['data_loaders'][-1])()]\n",
    "    pipeline = Pipeline(model,tokenizer,model_config,data_loaders)\n",
    "    # run\n",
    "    pipeline.run()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
